{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asinha13/anaconda3/envs/vclip/lib/python3.7/site-packages/mmcv/__init__.py:21: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  'On January 1, 2023, MMCV will release v2.0.0, in which it will remove '\n",
      "/home/asinha13/projects/CLIP4ADL/vifi_hyperformer/CLIP_ADL_AS/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from apex import amp\n",
    "from utils.logger import create_logger\n",
    "from datasets.build_AS import build_dataloader_no_ddp\n",
    "from trainers import vificlip_P2_merged_model_AS_20Oct23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(5)\n",
    "else:\n",
    "    print(\"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import get_config, _C\n",
    "\n",
    "config = _C\n",
    "# config.merge_from_file('./configs/base2novel/finetuning_base2novel/ntu/16_16_vifi_clip_base_train_55_5.yaml')\n",
    "config.merge_from_file('./configs/base2novel/finetuning_base2novel/ntu/16_16_vifi_clip_novel_eval_55_5.yaml')\n",
    "\n",
    "logger = create_logger(output_dir=config.OUTPUT, dist_rank=0, name=\"Test\")\n",
    "\n",
    "train_data, test_data, train_loader, test_loader = build_dataloader_no_ddp(logger='', config=config)\n",
    "class_names = [class_name for i, class_name in test_data.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returnclip hyperformer\n",
      "[2024-01-10 09:16:16 Test](vificlip_P2_merged_model_AS_20Oct23.py 316): INFO Loading CLIP (backbone: ViT-B/16)\n",
      "[2024-01-10 09:16:18 Test](vificlip_P2_merged_model_AS_20Oct23.py 319): INFO Building ViFi-CLIP CLIP\n",
      "vificlip hyperformer\n",
      "vificlip - before load_model hyperformer\n",
      "Hyperformer_Model(\n",
      "  (data_bn): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l1): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(3, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(3, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (skip_proj): Conv2d(3, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pe_proj): Conv2d(3, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "    (residual): unit_tcn(\n",
      "      (conv): Conv2d(3, 216, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (l2): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l3): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l4): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l5): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(2, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(2, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(2, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "    (residual): unit_tcn(\n",
      "      (conv): Conv2d(216, 216, kernel_size=(1, 1), stride=(2, 1))\n",
      "      (bn): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (l6): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l7): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l8): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(2, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(2, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(2, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "    (residual): unit_tcn(\n",
      "      (conv): Conv2d(216, 216, kernel_size=(1, 1), stride=(2, 1))\n",
      "      (bn): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (l9): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (l10): TCN_ViT_unit(\n",
      "    (vit1): unit_vit(\n",
      "      (norm1): LayerNorm((216,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHSA(\n",
      "        (kv): Conv2d(216, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (q): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), groups=6)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (pe_proj): Conv2d(216, 216, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn1): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): TemporalConv(\n",
      "            (conv): Conv2d(54, 54, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(216, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=216, out_features=512, bias=True)\n",
      ")\n",
      "[2024-01-10 09:16:18 Test](vificlip_P2_merged_model_AS_20Oct23.py 336): INFO Turning on gradients for COMPLETE ViFi-CLIP model\n",
      "[2024-01-10 09:16:18 Test](vificlip_P2_merged_model_AS_20Oct23.py 359): INFO Parameters to be updated: {'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l3.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.vit1.attn.proj.weight', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.ln_pre.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l5.residual.bn.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.class_embedding', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l2.vit1.attn.rpe', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'hyperformer_model.data_bn.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l3.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l6.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'text_encoder.text_projection', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l8.residual.bn.bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l7.vit1.attn.outer', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l9.vit1.attn.outer', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'hyperformer_model.l3.vit1.attn.q.weight', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.vit1.attn.w1', 'hyperformer_model.l2.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.norm1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l9.vit1.norm1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l2.vit1.attn.kv.weight', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.ln_final.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.residual.bn.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l7.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l5.vit1.attn.outer', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l8.vit1.attn.w1', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l4.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l10.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.data_bn.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.vit1.attn.alpha', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.vit1.attn.outer', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.attn.outer', 'hyperformer_model.l6.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l5.residual.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.positional_embedding', 'hyperformer_model.l6.vit1.norm1.bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'text_encoder.ln_final.bias', 'hyperformer_model.l2.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l4.vit1.attn.w1', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.fc2.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l5.vit1.attn.w1', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l9.vit1.attn.w1', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l9.vit1.attn.q.weight', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l7.vit1.attn.q.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'image_encoder.conv1.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l1.vit1.attn.w1', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'hyperformer_model.l5.residual.bn.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'image_encoder.ln_pre.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.alpha', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l8.residual.bn.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.proj', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l1.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias'}\n",
      "[2024-01-10 09:16:18 Test](vificlip_P2_merged_model_AS_20Oct23.py 360): INFO Total learnable items: 687\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/home/asinha13/anaconda3/envs/vclip/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK2at6Tensor6deviceEv')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asinha13/anaconda3/envs/vclip/lib/python3.7/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n"
     ]
    }
   ],
   "source": [
    "merged_model = vificlip_P2_merged_model_AS_20Oct23.returnCLIP(config,\n",
    "                            logger=logger,\n",
    "                            class_names=class_names,\n",
    "                            args='hyperformer')\n",
    "                            # args=args) # pass args to indicate that we want to load the hyperformer model\n",
    "\n",
    "merged_model = merged_model.cuda()\n",
    "\n",
    "merged_model = amp.initialize(models=merged_model, optimizers=None, opt_level=config.TRAIN.OPT_LEVEL)\n",
    "merged_model = merged_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_pose_txt_path = './work_dirs/zero_shot/ntu_CS_KD_vcp0_posetextp1_base_zs_55_5_AS_12Nov23_v1/ckpt_epoch_29.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['prompt_learner.complete_text_embeddings'], [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(p2_pose_txt_path, map_location='cpu')\n",
    "load_state_dict = checkpoint['model']\n",
    "\n",
    "# now remove the unwanted keys:\n",
    "if \"module.prompt_learner.token_prefix\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.token_prefix\"]\n",
    "\n",
    "if \"module.prompt_learner.token_suffix\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.token_suffix\"]\n",
    "\n",
    "if \"module.prompt_learner.complete_text_embeddings\" in load_state_dict:\n",
    "    del load_state_dict[\"module.prompt_learner.complete_text_embeddings\"]\n",
    "\n",
    "new_state_dict = {}\n",
    "for key in load_state_dict:\n",
    "    new_key = key.replace('module.', '')\n",
    "    new_state_dict[new_key] = load_state_dict[key]\n",
    "\n",
    "mk, uk = merged_model.load_state_dict(new_state_dict, strict=False)\n",
    "mk, uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2350\n",
      "label:  tensor([[0],\n",
      "        [3]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1309618/51477142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperformer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# text_features, pose_embd = model(image_input, hyperformer_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/CLIP4ADL/vifi_hyperformer/CLIP_ADL_AS/trainers/vificlip_P2_merged_model_AS_20Oct23.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, hyperformer_data)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Now pass the image into CLIP visual encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;31m# Now again attach the batch dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/CLIP4ADL/vifi_hyperformer/CLIP_ADL_AS/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, width, grid, grid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, width, grid ** 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape = [*, grid ** 2, width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/apex/amp/wrap.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vclip/lib/python3.7/site-packages/apex/amp/utils.py\u001b[0m in \u001b[0;36mcached_cast\u001b[0;34m(cast_fn, x, cache)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcached_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Make sure x is actually cached_x's autograd parent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcached_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 raise RuntimeError(\"x and cache[x] both require grad, but x is not \"\n\u001b[1;32m     99\u001b[0m                                    \"cache[x]'s parent.  This is likely an error.\")\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_samples, num_correct = 0, 0\n",
    "\n",
    "all_pose_text_logits_78 = None\n",
    "all_image_text_logits = None\n",
    "accuracy_meter = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    if i%(len(test_loader)//10) ==0:\n",
    "        print(f'{i}/{len(test_loader)}')\n",
    "        print('label: ', batch['label'])\n",
    "\n",
    "    _image = batch[\"imgs\"]\n",
    "    b, tn, c, h, w = _image.size()\n",
    "    t = config.DATA.NUM_FRAMES\n",
    "    n = tn // t\n",
    "    _image = _image.view(b, n, t, c, h, w)\n",
    "    hyperformer_data = batch[\"hformer_data\"].cuda(non_blocking=True)\n",
    "    label_id = batch[\"label\"]\n",
    "    label_id = label_id.reshape(-1)\n",
    "\n",
    "    tot_similarity = [torch.zeros((b, config.DATA.NUM_CLASSES)).cuda() for _ in range(3)]\n",
    "    for i in range(n):\n",
    "\n",
    "        image = _image[:, i, :, :, :, :]\n",
    "        label_id = label_id.cuda(non_blocking=True)\n",
    "        image_input = image.cuda(non_blocking=True)\n",
    "\n",
    "        image_features, text_features, pose_embd = merged_model(image_input, hyperformer_data)\n",
    "        # text_features, pose_embd = model(image_input, hyperformer_data)\n",
    "\n",
    "        image_text_logits = image_features @ text_features.t()\n",
    "        text_pose_logits = pose_embd @ text_features.t()\n",
    "        img_text_pose_logits = image_text_logits + text_pose_logits\n",
    "\n",
    "        # logits_list = [image_text_logits, text_pose_logits, img_text_pose_logits]\n",
    "        logits_list = [text_pose_logits]\n",
    "\n",
    "        for k, logits in enumerate(logits_list):\n",
    "            similarity = logits.view(b, -1).softmax(dim=-1)\n",
    "            tot_similarity[k] += similarity\n",
    "        \n",
    "        modality_names = ['text_pose']\n",
    "        \n",
    "        for k in range(len(modality_names)):\n",
    "            values_1, indices_1 = tot_similarity[k].topk(1, dim=-1)\n",
    "            values_5, indices_5 = tot_similarity[k].topk(5, dim=-1)\n",
    "            acc1, acc5 = 0, 0\n",
    "\n",
    "            conf_matrix = torch.zeros((config.DATA.NUM_CLASSES, config.DATA.NUM_CLASSES)).cuda()\n",
    "            for i in range(b):\n",
    "                conf_matrix[label_id[i].item(), indices_1[i].item()] += 1\n",
    "                if indices_1[i] == label_id[i]:\n",
    "                    acc1 += 1\n",
    "                if label_id[i] in indices_5[i]:\n",
    "                    acc5 += 1\n",
    "\n",
    "            \n",
    "            accuracy_meter.append(acc1 / b)\n",
    "            num_samples += b\n",
    "            num_correct += acc1\n",
    "\n",
    "average_accuracy = sum(accuracy_meter) / len(accuracy_meter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct, num_samples, num_correct/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2350\n",
      "label:  tensor([[0],\n",
      "        [3]])\n",
      "235/2350\n",
      "label:  tensor([[0],\n",
      "        [3]])\n",
      "470/2350\n",
      "label:  tensor([[0],\n",
      "        [1]])\n",
      "705/2350\n",
      "label:  tensor([[4],\n",
      "        [4]])\n",
      "940/2350\n",
      "label:  tensor([[0],\n",
      "        [0]])\n",
      "1175/2350\n",
      "label:  tensor([[1],\n",
      "        [1]])\n",
      "1410/2350\n",
      "label:  tensor([[2],\n",
      "        [0]])\n",
      "1645/2350\n",
      "label:  tensor([[4],\n",
      "        [1]])\n",
      "1880/2350\n",
      "label:  tensor([[1],\n",
      "        [3]])\n",
      "2115/2350\n",
      "label:  tensor([[0],\n",
      "        [4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2567), 4700, tensor(0.5462))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_correct = 0, 0\n",
    "\n",
    "all_pose_text_logits_78 = None\n",
    "all_image_text_logits = None\n",
    "for i, batch in enumerate(test_loader):\n",
    "    if i%(len(test_loader)//10) ==0:\n",
    "        print(f'{i}/{len(test_loader)}')\n",
    "        print('label: ', batch['label'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features, text_features, pose_embd = merged_model(batch['imgs'].cuda(), batch['hformer_data'].cuda())\n",
    "        # all_image_text_logits, text_pose_logits = merged_model_2(batch['imgs'].cuda(), batch['hformer_data'].cuda())\n",
    "\n",
    "        image_text_logits = image_features @ text_features.t()\n",
    "        text_pose_logits = pose_embd @ text_features.t()\n",
    "\n",
    "        logits = text_pose_logits.detach().cpu() # 2x60\n",
    "        # logits = (all_image_text_logits + text_pose_logits).detach().cpu() # 2x60\n",
    "        \n",
    "        if all_pose_text_logits_78 is None and all_image_text_logits is None:\n",
    "            all_pose_text_logits_78 = text_pose_logits\n",
    "            all_image_text_logits = image_text_logits\n",
    "        else:\n",
    "            all_pose_text_logits_78 = torch.cat([all_pose_text_logits_78, text_pose_logits], dim=0)\n",
    "            all_image_text_logits = torch.cat([all_image_text_logits, image_text_logits], dim=0)\n",
    "\n",
    "        # img_text_logits_P0.append(logits)\n",
    "        num_samples += logits.shape[0]\n",
    "        num_correct += (logits.argmax(dim=1)==batch['label'][:, 0]).sum()\n",
    "\n",
    "        # print(logits.argmax(dim=1), batch['label'][:, 0], f'{num_correct / num_samples}')\n",
    "\n",
    "num_correct, num_samples, num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
