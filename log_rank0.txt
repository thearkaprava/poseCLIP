[2024-01-10 00:42:45 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 00:42:50 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 00:42:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 00:42:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l8.residual.bn.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l2.vit1.norm1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'hyperformer_model.l1.vit1.attn.w1', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l4.vit1.attn.w1', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.data_bn.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.positional_embedding', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l7.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.ln_post.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.ln_final.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.data_bn.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.residual.bn.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.text_projection', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l8.vit1.attn.w1', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l1.residual.bn.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l5.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.class_embedding', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.proj', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.fc2.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'image_encoder.ln_pre.weight', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.conv1.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l9.vit1.attn.alpha', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l7.vit1.attn.outer', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.residual.conv.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l5.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.rpe', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l7.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l3.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l9.vit1.attn.outer', 'text_encoder.ln_final.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.vit1.attn.alpha', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l9.vit1.norm1.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l6.vit1.attn.w1', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.2.4.weight'}
[2024-01-10 00:42:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 00:50:54 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 00:50:59 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 00:50:59 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 00:50:59 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l8.residual.bn.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l2.vit1.norm1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'hyperformer_model.l1.vit1.attn.w1', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l4.vit1.attn.w1', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.data_bn.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.positional_embedding', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l7.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.ln_post.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.ln_final.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.data_bn.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.residual.bn.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.text_projection', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l8.vit1.attn.w1', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l1.residual.bn.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l5.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.class_embedding', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.proj', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.fc2.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'image_encoder.ln_pre.weight', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.conv1.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l9.vit1.attn.alpha', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l7.vit1.attn.outer', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.residual.conv.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l5.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.rpe', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l7.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l3.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l9.vit1.attn.outer', 'text_encoder.ln_final.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.vit1.attn.alpha', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l9.vit1.norm1.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l6.vit1.attn.w1', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.2.4.weight'}
[2024-01-10 00:50:59 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 00:51:09 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 00:51:14 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 00:51:14 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 00:51:14 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l8.residual.bn.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l2.vit1.norm1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'hyperformer_model.l1.vit1.attn.w1', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l4.vit1.attn.w1', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.data_bn.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.positional_embedding', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l7.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.ln_post.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.ln_final.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.data_bn.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.residual.bn.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.text_projection', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l8.vit1.attn.w1', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l1.residual.bn.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l5.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.class_embedding', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.proj', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.fc2.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'image_encoder.ln_pre.weight', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.conv1.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l10.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l9.vit1.attn.alpha', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l7.vit1.attn.outer', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.residual.conv.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l5.residual.bn.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.rpe', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l7.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l3.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l9.vit1.attn.outer', 'text_encoder.ln_final.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.vit1.attn.alpha', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l9.vit1.norm1.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l6.vit1.attn.w1', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.2.4.weight'}
[2024-01-10 00:51:14 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 00:53:46 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 00:53:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 00:53:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 00:53:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l3.vit1.norm1.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'hyperformer_model.l4.vit1.attn.rpe', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l1.vit1.skip_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l1.vit1.attn.kv.weight', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l6.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l2.vit1.attn.w1', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.ln_final.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.data_bn.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.ln_pre.weight', 'hyperformer_model.l6.vit1.attn.q.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'text_encoder.ln_final.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.vit1.attn.w1', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'image_encoder.proj', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l8.residual.bn.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l3.vit1.attn.outer', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l7.vit1.attn.proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l2.vit1.attn.rpe', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l6.vit1.attn.w1', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.vit1.norm1.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l7.vit1.attn.outer', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l7.vit1.attn.rpe', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.vit1.attn.proj.bias', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.attn.proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.positional_embedding', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'text_encoder.text_projection', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l6.vit1.norm1.weight', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'image_encoder.conv1.weight', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l2.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.data_bn.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l10.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l10.vit1.norm1.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.attn.w1', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l9.vit1.attn.alpha', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l4.vit1.attn.outer', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l8.vit1.norm1.weight', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'image_encoder.ln_pre.bias', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l1.residual.bn.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'image_encoder.positional_embedding', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.residual.conv.bias', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l2.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l1.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l8.residual.conv.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l6.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.attn.alpha', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l5.residual.bn.bias', 'image_encoder.class_embedding', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l8.vit1.pe_proj.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.fc2.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l5.residual.bn.weight', 'hyperformer_model.l5.vit1.attn.outer', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l6.vit1.attn.outer', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l5.vit1.attn.rpe', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'hyperformer_model.l1.residual.bn.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l9.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l8.vit1.attn.w1', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l9.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'hyperformer_model.l4.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'hyperformer_model.l2.vit1.attn.q.weight', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l7.vit1.attn.alpha'}
[2024-01-10 00:53:51 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 08:49:55 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 08:49:57 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 08:49:57 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 08:49:57 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l10.vit1.attn.alpha', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.text_projection', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l1.residual.bn.weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l7.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l1.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l1.vit1.attn.outer', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l8.vit1.norm1.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l9.vit1.attn.outer', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l8.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.fc2.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l7.vit1.attn.outer', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l4.vit1.attn.w1', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l7.vit1.attn.alpha', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l8.residual.conv.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l8.residual.bn.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l4.vit1.attn.rpe', 'image_encoder.ln_pre.weight', 'hyperformer_model.l3.vit1.norm1.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.vit1.norm1.weight', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l6.vit1.norm1.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l4.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l2.vit1.attn.outer', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.conv1.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l8.vit1.attn.w1', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l5.residual.conv.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'image_encoder.class_embedding', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l5.residual.bn.bias', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l10.vit1.attn.rpe', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'hyperformer_model.data_bn.weight', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.pe_proj.weight', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.data_bn.bias', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l1.residual.conv.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.residual.bn.weight', 'hyperformer_model.l5.vit1.attn.rpe', 'text_encoder.positional_embedding', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l6.vit1.attn.w1', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'text_encoder.ln_final.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l10.vit1.attn.w1', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l6.vit1.attn.alpha', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.w1', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l3.vit1.attn.outer', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.proj', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.ln_post.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l10.vit1.norm1.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.ln_final.bias', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l7.vit1.attn.kv.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l9.vit1.attn.rpe', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l9.vit1.norm1.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l6.vit1.norm1.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.fc2.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l10.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias'}
[2024-01-10 08:49:57 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 09:02:23 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 09:02:25 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 09:02:25 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 09:02:25 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l7.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l9.vit1.norm1.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.norm1.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l5.vit1.attn.w1', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.fc2.bias', 'text_encoder.ln_final.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'text_encoder.positional_embedding', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l9.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l8.residual.bn.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l1.vit1.skip_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'hyperformer_model.fc2.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l8.residual.bn.bias', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l6.vit1.attn.proj.weight', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l7.vit1.norm1.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'text_encoder.ln_final.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l6.vit1.attn.alpha', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l10.vit1.norm1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l7.vit1.attn.rpe', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l7.vit1.attn.outer', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l4.vit1.norm1.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l5.residual.bn.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l9.vit1.norm1.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.positional_embedding', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.attn.outer', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.vit1.attn.alpha', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l8.vit1.attn.w1', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l8.residual.conv.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l1.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l2.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.conv1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l3.vit1.norm1.weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l7.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.vit1.attn.rpe', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l8.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'hyperformer_model.l9.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l10.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.proj', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l4.vit1.attn.w1', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'hyperformer_model.l6.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'hyperformer_model.l5.residual.conv.bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.vit1.attn.outer', 'hyperformer_model.l5.residual.bn.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.ln_pre.weight', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l6.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'image_encoder.ln_post.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l7.vit1.attn.proj.bias', 'hyperformer_model.l8.residual.conv.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l10.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.data_bn.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'hyperformer_model.l5.vit1.attn.outer', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l1.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'text_encoder.text_projection', 'hyperformer_model.l1.residual.bn.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l1.vit1.attn.kv.weight', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.data_bn.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l7.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l2.vit1.norm1.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l10.vit1.norm1.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.class_embedding', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l3.vit1.norm1.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l5.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l7.vit1.attn.w1', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.vit1.attn.outer', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight'}
[2024-01-10 09:02:25 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 09:09:31 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 09:09:33 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 09:09:33 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 09:09:33 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.positional_embedding', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.ln_final.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.positional_embedding', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.ln_pre.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.ln_pre.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.conv1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.text_projection', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.proj', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.class_embedding', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.ln_post.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias'}
[2024-01-10 09:09:33 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 300
[2024-01-10 09:09:52 Test] (vificlip_P2_merged_model_AS_12Nov23.py 290): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 09:09:53 Test] (vificlip_P2_merged_model_AS_12Nov23.py 293): INFO Building ViFi-CLIP CLIP
[2024-01-10 09:09:53 Test] (vificlip_P2_merged_model_AS_12Nov23.py 310): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 09:09:53 Test] (vificlip_P2_merged_model_AS_12Nov23.py 333): INFO Parameters to be updated: {'image_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l3.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'hyperformer_model.l4.vit1.attn.alpha', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l4.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'hyperformer_model.fc2.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'hyperformer_model.l8.residual.bn.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l3.vit1.attn.outer', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l4.vit1.attn.outer', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l2.vit1.attn.kv.weight', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l9.vit1.norm1.weight', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.attn.proj.bias', 'hyperformer_model.data_bn.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.outer', 'hyperformer_model.l2.vit1.attn.proj.bias', 'image_encoder.conv1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l5.residual.bn.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'hyperformer_model.l9.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'hyperformer_model.l6.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l1.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.fc2.bias', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l1.vit1.attn.alpha', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l9.vit1.attn.w1', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l6.vit1.norm1.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l8.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'hyperformer_model.l10.vit1.pe_proj.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'text_encoder.positional_embedding', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.positional_embedding', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.ln_pre.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l9.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.vit1.attn.w1', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'hyperformer_model.l4.vit1.attn.rpe', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l9.vit1.attn.outer', 'hyperformer_model.l3.vit1.norm1.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.data_bn.bias', 'text_encoder.ln_final.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l5.residual.bn.weight', 'hyperformer_model.l10.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l7.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l6.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.text_projection', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l4.vit1.attn.w1', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l1.residual.bn.bias', 'image_encoder.class_embedding', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.ln_post.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l7.vit1.attn.alpha', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l2.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l4.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.vit1.attn.w1', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l2.vit1.norm1.weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l4.vit1.norm1.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l8.residual.conv.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'image_encoder.ln_pre.bias', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l6.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.vit1.attn.proj.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.vit1.attn.rpe', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l3.vit1.attn.alpha', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'image_encoder.proj', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l7.vit1.attn.outer', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.attn.q.weight', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l1.vit1.attn.w1', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l6.vit1.norm1.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l1.residual.bn.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'hyperformer_model.l2.vit1.norm1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l6.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l10.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l2.vit1.attn.w1', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l5.residual.conv.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l3.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l8.vit1.attn.w1', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l3.vit1.attn.w1', 'text_encoder.transformer.resblocks.3.ln_2.bias'}
[2024-01-10 09:09:53 Test] (vificlip_P2_merged_model_AS_12Nov23.py 334): INFO Total learnable items: 687
[2024-01-10 09:13:46 Test] (vificlip_P2_merged_model_AS_20Oct23.py 316): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 09:13:48 Test] (vificlip_P2_merged_model_AS_20Oct23.py 319): INFO Building ViFi-CLIP CLIP
[2024-01-10 09:16:16 Test] (vificlip_P2_merged_model_AS_20Oct23.py 316): INFO Loading CLIP (backbone: ViT-B/16)
[2024-01-10 09:16:18 Test] (vificlip_P2_merged_model_AS_20Oct23.py 319): INFO Building ViFi-CLIP CLIP
[2024-01-10 09:16:18 Test] (vificlip_P2_merged_model_AS_20Oct23.py 336): INFO Turning on gradients for COMPLETE ViFi-CLIP model
[2024-01-10 09:16:18 Test] (vificlip_P2_merged_model_AS_20Oct23.py 359): INFO Parameters to be updated: {'text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l10.vit1.attn.kv.weight', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l3.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l10.vit1.attn.proj.weight', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l5.vit1.norm1.bias', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.ln_pre.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l7.vit1.attn.w1', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l5.residual.bn.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.class_embedding', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'hyperformer_model.l2.vit1.attn.rpe', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'hyperformer_model.data_bn.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l1.vit1.attn.outer', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l3.vit1.attn.outer', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l6.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'text_encoder.text_projection', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.vit1.attn.outer', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'hyperformer_model.l8.residual.bn.bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.vit1.attn.w1', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l7.vit1.attn.outer', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'hyperformer_model.l1.residual.bn.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l9.vit1.attn.outer', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l8.vit1.norm1.weight', 'hyperformer_model.l3.vit1.attn.q.weight', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l10.vit1.attn.w1', 'hyperformer_model.l2.vit1.attn.alpha', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.vit1.norm1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.vit1.attn.alpha', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l9.vit1.norm1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l2.vit1.attn.kv.weight', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.ln_final.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'hyperformer_model.fc2.bias', 'hyperformer_model.l1.residual.bn.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l7.vit1.attn.kv.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'hyperformer_model.l5.vit1.attn.outer', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l4.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'hyperformer_model.l8.vit1.attn.w1', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'hyperformer_model.l4.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l10.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.norm1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.data_bn.bias', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l3.vit1.attn.alpha', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.vit1.attn.outer', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'hyperformer_model.l5.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l4.vit1.attn.outer', 'hyperformer_model.l6.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l5.residual.conv.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.positional_embedding', 'hyperformer_model.l6.vit1.norm1.bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'text_encoder.ln_final.bias', 'hyperformer_model.l2.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l8.residual.conv.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l4.vit1.attn.w1', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.fc2.weight', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l5.vit1.attn.w1', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l6.vit1.attn.proj.weight', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l9.vit1.attn.rpe', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l9.vit1.attn.w1', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'hyperformer_model.l9.vit1.attn.q.weight', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l9.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.rpe', 'hyperformer_model.l7.vit1.attn.q.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'image_encoder.conv1.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l1.vit1.attn.w1', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'hyperformer_model.l5.residual.bn.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'image_encoder.ln_post.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'image_encoder.ln_pre.weight', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l7.vit1.attn.alpha', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l8.residual.bn.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.proj', 'hyperformer_model.l5.vit1.attn.rpe', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l1.vit1.attn.kv.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l2.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias'}
[2024-01-10 09:16:18 Test] (vificlip_P2_merged_model_AS_20Oct23.py 360): INFO Total learnable items: 687
